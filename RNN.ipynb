{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1O3U41plbOh-hHESr6f9Dm8qAz7JVziNx","authorship_tag":"ABX9TyMJaS8d/EeF/le1ywjKAV4h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZwDKYNjVB2Z-","outputId":"093ca86f-6c43-4457-8531-820761081213"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"]}],"source":["!pip install transformers\n","import numpy as np\n","import pandas as pd\n","from transformers import AutoTokenizer, AutoModelForMaskedLM, AdamW,RobertaModel,RobertaTokenizer\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","import copy\n","import warnings\n","from sklearn.metrics import accuracy_score as acc\n","import torch.optim as optim\n","\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device\n","\n","tokenizer = RobertaTokenizer.from_pretrained(\"PlanTL-GOB-ES/roberta-base-bne\")"]},{"cell_type":"code","source":["sentiments=pd.read_csv(\"/content/drive/MyDrive/transformer/s.csv\")"],"metadata":{"id":"_snnpEZeLUo2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentiments=sentiments[[\"others\",\"joy\",\"sadness\",\"anger\",\"surprise\",\"disgust\",\"fear\"]]"],"metadata":{"id":"nwMliDX-LvEV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(sentiments)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uCdM1UE_MA71","executionInfo":{"status":"ok","timestamp":1684405044699,"user_tz":-120,"elapsed":2,"user":{"displayName":"Pepe Olivert","userId":"09214730254765494077"}},"outputId":"13c6d32d-e4b8-4fdc-a353-25d1161d66c1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2671"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":["data = pd.read_csv(\"/content/drive/MyDrive/transformer/train.csv\")\n","inputs = data[\"tweet\"]\n","labels = data[\"humor\"]\n","\n","\n","\n","train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n","    inputs, \n","    labels, \n","    test_size=0.2, \n","    stratify=labels\n",")\n"],"metadata":{"id":"qE7Ol36ICDeg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class createDataset(Dataset):\n","\n","    def __init__(self, texts, targets, tokenizer, max_len):\n","        self.texts = texts\n","        self.targets = targets\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","  \n","    def __len__(self):\n","        return len(self.texts)\n","  \n","    def __getitem__(self, item):\n","        text = str(self.texts[item])\n","        target = self.targets[item]\n","\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            pad_to_max_length=True,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","            truncation=True\n","        )\n","        input=encoding['input_ids'].flatten()\n","        \n","\n","        return {\n","            'text': text,\n","            'input_ids': input,\n","            \n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'targets': torch.tensor(target, dtype=torch.long)\n","        }\n","\n","def create_data_loader(texts, labels, tokenizer, max_len, batch_size):\n","    \n","    ds = createDataset(\n","        texts=texts.to_numpy(),\n","        targets=labels.to_numpy(),\n","        \n","        tokenizer=tokenizer,\n","        max_len=max_len\n","  )\n","\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        num_workers=4\n","      )"],"metadata":{"id":"XGqh4kKYCP90"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Model(nn.Module):\n","    def __init__(self, latent_dims,max_len,nhid):\n","        super(Model, self).__init__()\n","\n","        \n","        self.lstm = nn.LSTM(input_size=1,num_layers=1,hidden_size=128,dropout=0.5,batch_first=True)\n","        self.linear = nn.Linear(in_features=nhid*max_len, out_features=1028)\n","        self.r = nn.ReLU()\n","        self.t = nn.Tanh()\n","        self.l = nn.Linear(in_features=1028,out_features=256)\n","        self.l2=nn.Linear(in_features=256,out_features=2)\n","\n","        self.s = nn.Softmax(dim=1)\n","\n","        self.latent_dims=latent_dims\n","        self.nhid=nhid\n","\n","\n","\n","        \n","        \n","\n","    def forward(self, input_id,bs,ls):\n","\n","      input_id = input_id.to(torch.float32)\n","      input_id=input_id.reshape(bs,ls,1)\n","      out,(h,c)=self.lstm(input_id)\n","      out=out.flatten(-2)\n","      out=self.t(out)\n","      \n","\n","\n","      decoded = self.linear(out)\n","\n","      decoded=self.r(decoded)\n","\n","      decoded=self.l(decoded)\n","\n","      decoded=self.r(decoded)\n","\n","\n","      decoded=self.l2(decoded)\n","\n","\n","      \n","\n","      decoded=self.s(decoded)\n","\n","      \n","      \n","      \n","\n","     \n","      \n","      return decoded\n","\n","\n","\n","    def init_hidden(self, bsz):\n","        weight = next(self.parameters())\n","        return (weight.new_zeros(self.latent_dims, bsz, self.nhid),\n","                weight.new_zeros(self.latent_dims, bsz, self.nhid))\n","      \n","\n"],"metadata":{"id":"60_UHNBKGnbv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 64\n","learning_rate = 0.00001\n","criterion = nn.CrossEntropyLoss().to(device)\n","criterion.requires_grad=True\n","epochs = 30\n","latentdims=2\n","nhid=128\n","max_len=60\n"],"metadata":{"id":"yARD2D70YOfW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model= Model(latentdims,max_len,nhid)\n","model.to(device)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"eLaFw27XNtxs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data_loader = create_data_loader(train_inputs, train_labels,tokenizer, max_len, batch_size)\n","\n","test_data_loader = create_data_loader(test_inputs, test_labels, tokenizer, max_len, batch_size)"],"metadata":{"id":"LK5xnezKYtOV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import f1_score as f1"],"metadata":{"id":"4J0_lnsuvuKz","executionInfo":{"status":"ok","timestamp":1684411462144,"user_tz":-120,"elapsed":3,"user":{"displayName":"Pepe Olivert","userId":"09214730254765494077"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def train_an_epoch(\n","    model, \n","    train_data_loader,\n","    dev_data_loader,\n","    criterion, \n","    optimizer\n","):\n","\n","    \n","\n","    # These are the metrics that will indicate us how well it's doing the model...\n","    running_loss = 0\n","    training_acc=[]\n","    f1_training=[]\n","    steps = 0;\n","    \n","    for batch in train_data_loader:\n","        \n","        b=len(batch[\"input_ids\"])\n","        # Clean gradients...\n","        optimizer.zero_grad()\n","    \n","        # Get the information from the tokenization... (using GPU)\n","        input_ids = batch[\"input_ids\"].to(device)\n","        targets = batch[\"targets\"].to(device)\n","\n","        # get the model's predictions...\n","        outputs = model(\n","            input_id=input_ids,\n","            bs=b,\n","            ls=60  \n","        )\n","\n","        # Apply the loss function and the perform backward propagation...\n","       \n","        loss = criterion(outputs, targets)\n","       \n","        loss.backward() \n","        optimizer.step()\n","        \n","        # update the metrics...\n","\n","        pred = []\n","        real=[]\n","        for output in outputs:\n","          zero=output[0].item()\n","          one=output[1].item()\n","          if zero > one:\n","            pred.append(0)\n","          else:\n","            pred.append(1)\n","\n","        for t in targets:\n","\n","          real.append(t.item())\n","\n","        bacc=acc(real,pred)\n","        bf1= f1(real,pred)\n","        running_loss+=loss.item()\n","        training_acc.append(bacc) \n","        f1_training.append(bf1)\n","\n","        steps+=1\n","            \n","    # get the mean of the metrics...\n","    \n","    loss = running_loss/steps;\n","    t_acc=sum(training_acc)/len(training_acc)\n","    t_f1=sum(f1_training)/len(f1_training)\n","    \n","    #acc = running_accs/steps;\n","    \n","    # evaluate the model with the validation data set \n","    # (\"turn off\" gradients...)\n","    with torch.no_grad():\n","        \n","        # These are the metrics that will indicate us how well it's doing the model...\n","        test_acc=[];\n","        steps_val=0;\n","        f1_test=[]\n","        \n","        for batch in dev_data_loader:\n","\n","            b= len(batch[\"input_ids\"])\n","            \n","            # Get the information from the tokenization... (using GPU)\n","            input_ids = batch[\"input_ids\"].to(device)\n","            targets = batch[\"targets\"].to(device)\n","\n","            # get the model's predictions...\n","            outputs = model(\n","                input_id=input_ids,\n","                bs=b,\n","                ls=60\n","                \n","            )\n","            \n","            pred = []\n","            real=[]\n","            for output in outputs:\n","              zero=output[0].item()\n","              one=output[1].item()\n","              if zero > one:\n","                pred.append(0)\n","              else:\n","                pred.append(1)\n","\n","            for t in targets:\n","\n","              real.append(t.item())\n","\n","            bacc=acc(real,pred)\n","            bf1= f1(real,pred)\n","            test_acc.append(bacc)\n","            f1_test.append(bf1)\n","\n","\n","        v_acc=sum(test_acc)/len(test_acc)\n","        v_f1= sum(f1_test)/len(f1_test)\n","    \n","    \n","\n","    return loss,t_acc,v_acc,t_f1,v_f1\n","\n","def train_the_model(epochs):\n","    \n","    for e in range(epochs):\n","      #, acc, val_acc\n","        \n","        loss,t_acc,v_acc,t_f1,v_f1 = train_an_epoch(\n","            model, \n","            train_data_loader,\n","            test_data_loader,\n","            criterion, \n","            optimizer\n","        )\n","        \n","        print('--------EPOCH SUMMARY---------')\n","        print('Epoch ', e+1, ' training loss: ', loss)\n","        print('Epoch ', e+1, ' training acc: ', t_acc*100, '%')\n","        print('Epoch ', e+1, ' val acc: ', v_acc*100, '%')\n","        print('Epoch ', e+1, ' training f1: ', t_f1*100, '%')\n","        print('Epoch ', e+1, ' val f1: ', v_f1*100, '%')"],"metadata":{"id":"6SWujgeMMsrp","executionInfo":{"status":"ok","timestamp":1684411464690,"user_tz":-120,"elapsed":365,"user":{"displayName":"Pepe Olivert","userId":"09214730254765494077"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["train_the_model(epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t52Es1ihOXIu","outputId":"2a0c1932-5cd9-4e09-b691-d8a561f3345b","executionInfo":{"status":"ok","timestamp":1684411556298,"user_tz":-120,"elapsed":89661,"user":{"displayName":"Pepe Olivert","userId":"09214730254765494077"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["--------EPOCH SUMMARY---------\n","Epoch  1  training loss:  0.6537026762962341\n","Epoch  1  training acc:  67.67769607843137 %\n","Epoch  1  val acc:  67.93478260869566 %\n","Epoch  1  training f1:  0.0 %\n","Epoch  1  val f1:  0.0 %\n","--------EPOCH SUMMARY---------\n","Epoch  2  training loss:  0.6316905652775484\n","Epoch  2  training acc:  67.67769607843137 %\n","Epoch  2  val acc:  67.93478260869566 %\n","Epoch  2  training f1:  0.0 %\n","Epoch  2  val f1:  0.0 %\n","--------EPOCH SUMMARY---------\n","Epoch  3  training loss:  0.6252783729749567\n","Epoch  3  training acc:  67.67769607843137 %\n","Epoch  3  val acc:  67.93478260869566 %\n","Epoch  3  training f1:  0.0 %\n","Epoch  3  val f1:  0.0 %\n","--------EPOCH SUMMARY---------\n","Epoch  4  training loss:  0.6200393648708568\n","Epoch  4  training acc:  67.67769607843137 %\n","Epoch  4  val acc:  67.93478260869566 %\n","Epoch  4  training f1:  0.0 %\n","Epoch  4  val f1:  0.0 %\n","--------EPOCH SUMMARY---------\n","Epoch  5  training loss:  0.6156783331842983\n","Epoch  5  training acc:  67.67769607843137 %\n","Epoch  5  val acc:  67.93478260869566 %\n","Epoch  5  training f1:  0.0 %\n","Epoch  5  val f1:  0.0 %\n","--------EPOCH SUMMARY---------\n","Epoch  6  training loss:  0.6116333779166726\n","Epoch  6  training acc:  68.03002450980392 %\n","Epoch  6  val acc:  68.45561594202898 %\n","Epoch  6  training f1:  2.522281639928699 %\n","Epoch  6  val f1:  3.02466310704297 %\n","--------EPOCH SUMMARY---------\n","Epoch  7  training loss:  0.6072463042595807\n","Epoch  7  training acc:  68.71936274509804 %\n","Epoch  7  val acc:  69.36141304347827 %\n","Epoch  7  training f1:  8.076269898520538 %\n","Epoch  7  val f1:  10.863315696649028 %\n","--------EPOCH SUMMARY---------\n","Epoch  8  training loss:  0.6033502449007595\n","Epoch  8  training acc:  69.1329656862745 %\n","Epoch  8  val acc:  70.09359903381642 %\n","Epoch  8  training f1:  13.940701059917068 %\n","Epoch  8  val f1:  18.701639079450175 %\n","--------EPOCH SUMMARY---------\n","Epoch  9  training loss:  0.5987714914714589\n","Epoch  9  training acc:  69.59252450980392 %\n","Epoch  9  val acc:  70.57669082125604 %\n","Epoch  9  training f1:  18.026909422294953 %\n","Epoch  9  val f1:  22.32884399551066 %\n","--------EPOCH SUMMARY---------\n","Epoch  10  training loss:  0.5940119483891655\n","Epoch  10  training acc:  70.17463235294117 %\n","Epoch  10  val acc:  71.1352657004831 %\n","Epoch  10  training f1:  21.923971669172026 %\n","Epoch  10  val f1:  27.360220449999094 %\n","--------EPOCH SUMMARY---------\n","Epoch  11  training loss:  0.5886068572016323\n","Epoch  11  training acc:  71.0171568627451 %\n","Epoch  11  val acc:  70.78804347826087 %\n","Epoch  11  training f1:  26.718674684886185 %\n","Epoch  11  val f1:  25.75328857937554 %\n","--------EPOCH SUMMARY---------\n","Epoch  12  training loss:  0.5833041071891785\n","Epoch  12  training acc:  71.875 %\n","Epoch  12  val acc:  71.1352657004831 %\n","Epoch  12  training f1:  31.329792397781432 %\n","Epoch  12  val f1:  27.879760673589566 %\n","--------EPOCH SUMMARY---------\n","Epoch  13  training loss:  0.5781990403638166\n","Epoch  13  training acc:  72.24264705882352 %\n","Epoch  13  val acc:  71.3088768115942 %\n","Epoch  13  training f1:  35.545165282324284 %\n","Epoch  13  val f1:  29.516793119734295 %\n","--------EPOCH SUMMARY---------\n","Epoch  14  training loss:  0.5736297360237907\n","Epoch  14  training acc:  72.84007352941177 %\n","Epoch  14  val acc:  72.31280193236714 %\n","Epoch  14  training f1:  39.372168121673226 %\n","Epoch  14  val f1:  34.99343866010533 %\n","--------EPOCH SUMMARY---------\n","Epoch  15  training loss:  0.569745429298457\n","Epoch  15  training acc:  73.16176470588235 %\n","Epoch  15  val acc:  73.00724637681158 %\n","Epoch  15  training f1:  40.996762419037694 %\n","Epoch  15  val f1:  38.48188437077326 %\n","--------EPOCH SUMMARY---------\n","Epoch  16  training loss:  0.5666027402176577\n","Epoch  16  training acc:  73.39154411764706 %\n","Epoch  16  val acc:  73.00724637681158 %\n","Epoch  16  training f1:  42.73578255855806 %\n","Epoch  16  val f1:  39.04155515266626 %\n","--------EPOCH SUMMARY---------\n","Epoch  17  training loss:  0.5639692553702522\n","Epoch  17  training acc:  73.98897058823529 %\n","Epoch  17  val acc:  73.00724637681158 %\n","Epoch  17  training f1:  45.133415800377655 %\n","Epoch  17  val f1:  40.33769255991479 %\n","--------EPOCH SUMMARY---------\n","Epoch  18  training loss:  0.5617120160776026\n","Epoch  18  training acc:  74.21875 %\n","Epoch  18  val acc:  73.35446859903381 %\n","Epoch  18  training f1:  46.368430341485634 %\n","Epoch  18  val f1:  41.80231507716888 %\n","--------EPOCH SUMMARY---------\n","Epoch  19  training loss:  0.5597372554680881\n","Epoch  19  training acc:  74.54044117647058 %\n","Epoch  19  val acc:  73.70169082125604 %\n","Epoch  19  training f1:  47.43034724345963 %\n","Epoch  19  val f1:  43.078440682796405 %\n","--------EPOCH SUMMARY---------\n","Epoch  20  training loss:  0.5579658176969079\n","Epoch  20  training acc:  74.58639705882352 %\n","Epoch  20  val acc:  73.70169082125604 %\n","Epoch  20  training f1:  47.96465818474876 %\n","Epoch  20  val f1:  43.49866068032452 %\n","--------EPOCH SUMMARY---------\n","Epoch  21  training loss:  0.5563468372120577\n","Epoch  21  training acc:  74.58639705882352 %\n","Epoch  21  val acc:  73.52807971014492 %\n","Epoch  21  training f1:  48.369834700109244 %\n","Epoch  21  val f1:  43.26474255166955 %\n","--------EPOCH SUMMARY---------\n","Epoch  22  training loss:  0.554877948235063\n","Epoch  22  training acc:  74.63235294117648 %\n","Epoch  22  val acc:  73.70169082125604 %\n","Epoch  22  training f1:  48.64359383132532 %\n","Epoch  22  val f1:  43.81432439157397 %\n","--------EPOCH SUMMARY---------\n","Epoch  23  training loss:  0.5535166333703434\n","Epoch  23  training acc:  74.90808823529412 %\n","Epoch  23  val acc:  73.52807971014492 %\n","Epoch  23  training f1:  49.4225591990617 %\n","Epoch  23  val f1:  44.200797821525654 %\n","--------EPOCH SUMMARY---------\n","Epoch  24  training loss:  0.5522444774122799\n","Epoch  24  training acc:  75.0 %\n","Epoch  24  val acc:  73.70169082125604 %\n","Epoch  24  training f1:  49.81077309926089 %\n","Epoch  24  val f1:  44.695945375496734 %\n","--------EPOCH SUMMARY---------\n","Epoch  25  training loss:  0.5510077196009019\n","Epoch  25  training acc:  75.22977941176471 %\n","Epoch  25  val acc:  73.70169082125604 %\n","Epoch  25  training f1:  50.55093280554486 %\n","Epoch  25  val f1:  44.695945375496734 %\n","--------EPOCH SUMMARY---------\n","Epoch  26  training loss:  0.5498226959915722\n","Epoch  26  training acc:  75.32169117647058 %\n","Epoch  26  val acc:  74.22252415458937 %\n","Epoch  26  training f1:  50.79303782744371 %\n","Epoch  26  val f1:  46.2215462114424 %\n","--------EPOCH SUMMARY---------\n","Epoch  27  training loss:  0.5486531766021953\n","Epoch  27  training acc:  75.41360294117648 %\n","Epoch  27  val acc:  74.22252415458937 %\n","Epoch  27  training f1:  51.21851278652593 %\n","Epoch  27  val f1:  46.2215462114424 %\n","--------EPOCH SUMMARY---------\n","Epoch  28  training loss:  0.5474897596765967\n","Epoch  28  training acc:  75.64338235294117 %\n","Epoch  28  val acc:  74.22252415458937 %\n","Epoch  28  training f1:  51.85554368246576 %\n","Epoch  28  val f1:  46.2215462114424 %\n","--------EPOCH SUMMARY---------\n","Epoch  29  training loss:  0.5463341562186971\n","Epoch  29  training acc:  75.87316176470588 %\n","Epoch  29  val acc:  74.22252415458937 %\n","Epoch  29  training f1:  52.83123998112853 %\n","Epoch  29  val f1:  46.2215462114424 %\n","--------EPOCH SUMMARY---------\n","Epoch  30  training loss:  0.5451927079873926\n","Epoch  30  training acc:  75.87316176470588 %\n","Epoch  30  val acc:  74.22252415458937 %\n","Epoch  30  training f1:  53.06657374341537 %\n","Epoch  30  val f1:  46.52957348254268 %\n"]}]}]}